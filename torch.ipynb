{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1586c35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-1.12.1-cp38-cp38-manylinux1_x86_64.whl (776.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 776.3 MB 700 bytes/s  0:00:016\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /home/vladlen/anaconda3/lib/python3.8/site-packages (from torch) (3.7.4.3)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-1.12.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6e406466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.4076e-23,  3.0766e-41, -7.3267e-15],\n",
       "        [ 4.5633e-41,  4.4842e-44,  0.0000e+00]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "x = torch.Tensor(2, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ad2d737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6569, 0.5412, 0.2112],\n",
       "        [0.8432, 0.2255, 0.1807]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(2, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "01ab97eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 3., 2.],\n",
       "        [2., 3., 2.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.ones(2, 3)\n",
    "y = y*2\n",
    "y[:,1] += 1\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "127e5d79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[13., 13.],\n",
       "        [13., 13.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Variable(torch.ones(2, 2)*2, requires_grad = True)\n",
    "z = 2 * (x * x) + 5 * x\n",
    "z.backward(torch.ones(2, 2))\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb2c159",
   "metadata": {},
   "source": [
    "# Простейшая нейронная сеть на датасете MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4181b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51500004",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_gradient():\n",
    "    # print the gradient of 2x^2 + 5x\n",
    "    x = Variable(torch.ones(2, 2) * 2, requires_grad=True)\n",
    "    z = 2 * (x * x) + 5 * x\n",
    "    # run the backpropagation\n",
    "    z.backward(torch.ones(2, 2))\n",
    "    print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0f4e6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nn(batch_size=200, learning_rate=0.01, epochs=3, log_interval=10):\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])),\n",
    "        batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    \n",
    "    class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            self.fc1 = nn.Linear(28 * 28, 200)\n",
    "            self.fc2 = nn.Linear(200, 200)\n",
    "            self.fc3 = nn.Linear(200, 10)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = self.fc3(x)\n",
    "            return F.log_softmax(x)\n",
    "        \n",
    "        \n",
    "    net = Net()\n",
    "    print(net)\n",
    "\n",
    "    # create a stochastic gradient descent optimizer\n",
    "    optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    # create a loss function\n",
    "    criterion = nn.NLLLoss()\n",
    "    \n",
    "    \n",
    "     # run the main training loop\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            # resize data from (batch_size, 1, 28, 28) to (batch_size, 28*28)\n",
    "            data = data.view(-1, 28*28)\n",
    "            optimizer.zero_grad()\n",
    "            net_out = net(data)\n",
    "            loss = criterion(net_out, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % log_interval == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                           100. * batch_idx / len(train_loader), loss.item()))\n",
    "                \n",
    "    \n",
    "    \n",
    "    # run a test loop\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        data = data.view(-1, 28 * 28)\n",
    "        net_out = net(data)\n",
    "        # sum up batch loss\n",
    "        test_loss += criterion(net_out, target).item()\n",
    "        pred = net_out.data.max(1)[1]  # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data).sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1755dc8d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=200, bias=True)\n",
      "  (fc2): Linear(in_features=200, out_features=200, bias=True)\n",
      "  (fc3): Linear(in_features=200, out_features=10, bias=True)\n",
      ")\n",
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.302495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-60c11e94d64a>:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [2000/60000 (3%)]\tLoss: 2.211121\n",
      "Train Epoch: 0 [4000/60000 (7%)]\tLoss: 1.942721\n",
      "Train Epoch: 0 [6000/60000 (10%)]\tLoss: 1.514886\n",
      "Train Epoch: 0 [8000/60000 (13%)]\tLoss: 0.830566\n",
      "Train Epoch: 0 [10000/60000 (17%)]\tLoss: 0.585620\n",
      "Train Epoch: 0 [12000/60000 (20%)]\tLoss: 0.429515\n",
      "Train Epoch: 0 [14000/60000 (23%)]\tLoss: 0.542018\n",
      "Train Epoch: 0 [16000/60000 (27%)]\tLoss: 0.347749\n",
      "Train Epoch: 0 [18000/60000 (30%)]\tLoss: 0.442372\n",
      "Train Epoch: 0 [20000/60000 (33%)]\tLoss: 0.287191\n",
      "Train Epoch: 0 [22000/60000 (37%)]\tLoss: 0.310567\n",
      "Train Epoch: 0 [24000/60000 (40%)]\tLoss: 0.353946\n",
      "Train Epoch: 0 [26000/60000 (43%)]\tLoss: 0.290606\n",
      "Train Epoch: 0 [28000/60000 (47%)]\tLoss: 0.318203\n",
      "Train Epoch: 0 [30000/60000 (50%)]\tLoss: 0.193455\n",
      "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.348322\n",
      "Train Epoch: 0 [34000/60000 (57%)]\tLoss: 0.373362\n",
      "Train Epoch: 0 [36000/60000 (60%)]\tLoss: 0.332179\n",
      "Train Epoch: 0 [38000/60000 (63%)]\tLoss: 0.250362\n",
      "Train Epoch: 0 [40000/60000 (67%)]\tLoss: 0.327004\n",
      "Train Epoch: 0 [42000/60000 (70%)]\tLoss: 0.172202\n",
      "Train Epoch: 0 [44000/60000 (73%)]\tLoss: 0.294102\n",
      "Train Epoch: 0 [46000/60000 (77%)]\tLoss: 0.353113\n",
      "Train Epoch: 0 [48000/60000 (80%)]\tLoss: 0.256753\n",
      "Train Epoch: 0 [50000/60000 (83%)]\tLoss: 0.277095\n",
      "Train Epoch: 0 [52000/60000 (87%)]\tLoss: 0.166650\n",
      "Train Epoch: 0 [54000/60000 (90%)]\tLoss: 0.311642\n",
      "Train Epoch: 0 [56000/60000 (93%)]\tLoss: 0.294226\n",
      "Train Epoch: 0 [58000/60000 (97%)]\tLoss: 0.223312\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.222345\n",
      "Train Epoch: 1 [2000/60000 (3%)]\tLoss: 0.225022\n",
      "Train Epoch: 1 [4000/60000 (7%)]\tLoss: 0.235978\n",
      "Train Epoch: 1 [6000/60000 (10%)]\tLoss: 0.213011\n",
      "Train Epoch: 1 [8000/60000 (13%)]\tLoss: 0.187734\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 0.205973\n",
      "Train Epoch: 1 [12000/60000 (20%)]\tLoss: 0.164898\n",
      "Train Epoch: 1 [14000/60000 (23%)]\tLoss: 0.227096\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.217340\n",
      "Train Epoch: 1 [18000/60000 (30%)]\tLoss: 0.273047\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 0.184906\n",
      "Train Epoch: 1 [22000/60000 (37%)]\tLoss: 0.177961\n",
      "Train Epoch: 1 [24000/60000 (40%)]\tLoss: 0.211594\n",
      "Train Epoch: 1 [26000/60000 (43%)]\tLoss: 0.182653\n",
      "Train Epoch: 1 [28000/60000 (47%)]\tLoss: 0.264617\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 0.196447\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.191693\n",
      "Train Epoch: 1 [34000/60000 (57%)]\tLoss: 0.173755\n",
      "Train Epoch: 1 [36000/60000 (60%)]\tLoss: 0.275164\n",
      "Train Epoch: 1 [38000/60000 (63%)]\tLoss: 0.151557\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 0.169991\n",
      "Train Epoch: 1 [42000/60000 (70%)]\tLoss: 0.176087\n",
      "Train Epoch: 1 [44000/60000 (73%)]\tLoss: 0.156411\n",
      "Train Epoch: 1 [46000/60000 (77%)]\tLoss: 0.191942\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.131275\n",
      "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 0.183587\n",
      "Train Epoch: 1 [52000/60000 (87%)]\tLoss: 0.187682\n",
      "Train Epoch: 1 [54000/60000 (90%)]\tLoss: 0.182340\n",
      "Train Epoch: 1 [56000/60000 (93%)]\tLoss: 0.135304\n",
      "Train Epoch: 1 [58000/60000 (97%)]\tLoss: 0.148866\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.109978\n",
      "Train Epoch: 2 [2000/60000 (3%)]\tLoss: 0.136892\n",
      "Train Epoch: 2 [4000/60000 (7%)]\tLoss: 0.132577\n",
      "Train Epoch: 2 [6000/60000 (10%)]\tLoss: 0.141058\n",
      "Train Epoch: 2 [8000/60000 (13%)]\tLoss: 0.197953\n",
      "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 0.237175\n",
      "Train Epoch: 2 [12000/60000 (20%)]\tLoss: 0.257446\n",
      "Train Epoch: 2 [14000/60000 (23%)]\tLoss: 0.122904\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.191233\n",
      "Train Epoch: 2 [18000/60000 (30%)]\tLoss: 0.109669\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 0.218672\n",
      "Train Epoch: 2 [22000/60000 (37%)]\tLoss: 0.131416\n",
      "Train Epoch: 2 [24000/60000 (40%)]\tLoss: 0.137026\n",
      "Train Epoch: 2 [26000/60000 (43%)]\tLoss: 0.115587\n",
      "Train Epoch: 2 [28000/60000 (47%)]\tLoss: 0.085602\n",
      "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 0.079425\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.179692\n",
      "Train Epoch: 2 [34000/60000 (57%)]\tLoss: 0.226409\n",
      "Train Epoch: 2 [36000/60000 (60%)]\tLoss: 0.172744\n",
      "Train Epoch: 2 [38000/60000 (63%)]\tLoss: 0.135065\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.095487\n",
      "Train Epoch: 2 [42000/60000 (70%)]\tLoss: 0.119970\n",
      "Train Epoch: 2 [44000/60000 (73%)]\tLoss: 0.156518\n",
      "Train Epoch: 2 [46000/60000 (77%)]\tLoss: 0.067042\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.104609\n",
      "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 0.057196\n",
      "Train Epoch: 2 [52000/60000 (87%)]\tLoss: 0.101784\n",
      "Train Epoch: 2 [54000/60000 (90%)]\tLoss: 0.083276\n",
      "Train Epoch: 2 [56000/60000 (93%)]\tLoss: 0.110988\n",
      "Train Epoch: 2 [58000/60000 (97%)]\tLoss: 0.130260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-60c11e94d64a>:64: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  data, target = Variable(data, volatile=True), Variable(target)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0006, Accuracy: 9634/10000 (96%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run_opt = 2\n",
    "    if run_opt == 1:\n",
    "        simple_gradient()\n",
    "    elif run_opt == 2:\n",
    "        create_nn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9420787f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
